{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f1df22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. dataset of sentences to overall sentiment\n",
    "2. create a model from dataset of word to sentiment\n",
    "2.5 gramatically break down sentence instead of word by word\n",
    "3. use model for our own sentence input to output overall sentiment of sentence\n",
    "3.5 if a word is not known --> do sentiment analysis on its dictionary definition or neutralize\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6d84cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk.data\n",
    "import spacy\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "\n",
    "# need to run first 'python -m spacy download en_core_web_sm'\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "\n",
    "lm = WordNetLemmatizer()\n",
    "\n",
    "def normalize(d):\n",
    "    mult = 1.0/sum(d.values())\n",
    "    for key in d:\n",
    "        d[key] *= mult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9abba92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lindseychin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lindseychin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea73b282",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Viterbi Algorithm -- Code taken from Wikipedia \"\"\"\n",
    "\n",
    "def viterbi(obs, states, start_p, trans_p, emit_p):\n",
    "    V = [{}]\n",
    "    for st in states:\n",
    "        V[0][st] = {\"prob\": start_p[st] * emit_p[st][obs[0]], \"prev\": None}\n",
    "    # Run Viterbi when t > 0\n",
    "    for t in range(1, len(obs)):\n",
    "        V.append({})\n",
    "        for st in states:\n",
    "            max_tr_prob = V[t - 1][states[0]][\"prob\"] * trans_p[states[0]][st]\n",
    "            prev_st_selected = states[0]\n",
    "            for prev_st in states[1:]:\n",
    "                tr_prob = V[t - 1][prev_st][\"prob\"] * trans_p[prev_st][st]\n",
    "                if tr_prob > max_tr_prob:\n",
    "                    max_tr_prob = tr_prob\n",
    "                    prev_st_selected = prev_st\n",
    "\n",
    "            if obs[t] in emit_p[st]:\n",
    "                max_prob = max_tr_prob * emit_p[st][obs[t]]\n",
    "            V[t][st] = {\"prob\": max_prob, \"prev\": prev_st_selected}\n",
    "\n",
    "    opt = []\n",
    "    max_prob = -1\n",
    "    best_st = None\n",
    "    # Get most probable state and its backtrack\n",
    "    print(V)\n",
    "    for st, data in V[-1].items():\n",
    "        if data[\"prob\"] > max_prob:\n",
    "            max_prob = data[\"prob\"]\n",
    "            best_st = st\n",
    "    opt.append(best_st)\n",
    "    previous = best_st\n",
    "\n",
    "    # Follow the backtrack till the first observation\n",
    "    for t in range(len(V) - 2, -1, -1):\n",
    "        opt.insert(0, V[t + 1][previous][\"prev\"])\n",
    "        previous = V[t + 1][previous][\"prev\"]\n",
    "\n",
    "    return (opt, max_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "704c14bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [\"sentiment\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "df = pd.read_csv(\n",
    "    'training.1600000.processed.noemoticon.csv',\n",
    "    header=None, encoding=\"ISO-8859-1\", names=col_names)\n",
    "\n",
    "# normalize to 0 to 1 values\n",
    "df['sentiment'] = df['sentiment'].replace(4, 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, df['sentiment'], test_size = 0.005, random_state=0)\n",
    "\n",
    "df = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fac3fa9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NEGATION_WORDS = ['not', 'no']\n",
    "STOP_WORDS = stopwords.words('english')\n",
    "\n",
    "for word in NEGATION_WORDS:\n",
    "    STOP_WORDS.remove(word)\n",
    "\n",
    "URL_PATTERN = r'((https://[^ ]*|(http://)[^ ]*|( www\\.)[^ ]*))'\n",
    "USER_PATTERN = '@[^\\s]+'\n",
    "PUNCTUATIONS = ['!', '?', '&quot;']\n",
    "\n",
    "processed = []\n",
    "\n",
    "# NOTE TO SELF: REMOVE PUNCTUATION BC SPACY\n",
    "for sentiment, tweet in zip(df['sentiment'], df['text']):\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(URL_PATTERN, '', tweet)\n",
    "    tweet = re.sub(USER_PATTERN, '', tweet)\n",
    "    for p in PUNCTUATIONS:\n",
    "        tweet = tweet.replace(p, '')\n",
    "    for sw in STOP_WORDS:\n",
    "        tweet = re.sub(r'\\b{0}\\b'.format(sw), '', tweet)\n",
    "    for w in NEGATION_WORDS:\n",
    "        tweet = re.sub(r'\\b{0} \\b'.format(w), '{0}_'.format(w), tweet)\n",
    "        \n",
    "    processed.append((sentiment, tweet))    \n",
    "    \n",
    "df = pd.DataFrame(data=processed, columns=['sentiment', 'tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b9c7e38",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"NAIVE MODEL\"\"\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, df['sentiment'], test_size = 0.05, random_state=0)\n",
    "\n",
    "model = {}\n",
    "for sentiment, tweet in zip(X_train['sentiment'], X_train['tweet']):\n",
    "    for word in tweet.split():\n",
    "        word = lm.lemmatize(word)\n",
    "        if word in model:\n",
    "            count, avg = model[word]\n",
    "            model[word] = (count+1, (count*avg+sentiment) / (count+1))\n",
    "        else:\n",
    "            model[word] = (1, sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95576816",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"SUPERVISED TRANSITON + EMISSION MATRICES\"\"\"\n",
    "pos_tweets = X_train[X_train[\"sentiment\"] == 1]\n",
    "transition = {\n",
    "    'NOUN': { 'NOUN': 0, 'POS_VERB': 0, 'POS_ADJ': 0, 'NEG_VERB': 0, 'NEG_ADJ': 0},\n",
    "    'POS_VERB': { 'NOUN': 0, 'POS_VERB': 0, 'POS_ADJ': 0, 'NEG_VERB': 0, 'NEG_ADJ': 0},\n",
    "    'POS_ADJ': { 'NOUN': 0, 'POS_VERB': 0, 'POS_ADJ': 0, 'NEG_VERB': 0, 'NEG_ADJ': 0},\n",
    "    'NEG_VERB': { 'NOUN': 0, 'POS_VERB': 0, 'POS_ADJ': 0, 'NEG_VERB': 0, 'NEG_ADJ': 0},\n",
    "    'NEG_ADJ': { 'NOUN': 0, 'POS_VERB': 0, 'POS_ADJ': 0, 'NEG_VERB': 0, 'NEG_ADJ': 0}\n",
    "}\n",
    "\n",
    "emission = {\n",
    "    'NOUN': {},\n",
    "    'POS_VERB': {},\n",
    "    'POS_ADJ': {},\n",
    "    'NEG_VERB': {},\n",
    "    'NEG_ADJ': {}\n",
    "}\n",
    "\n",
    "VALID_POS = set(['NOUN', 'ADJ', 'VERB', 'PRON', 'PROPN'])\n",
    "\n",
    "#name pending\n",
    "def remap_pos(pos, val):\n",
    "    if pos == 'NOUN' or pos == 'PRON' or pos == 'PROPN':\n",
    "        return 'NOUN'\n",
    "    prefix = 'POS_' if val >= 0.5 else 'NEG_'\n",
    "    return prefix + pos\n",
    "\n",
    "for tweet in pos_tweets['tweet']:\n",
    "    sentence = sp(tweet)\n",
    "    cleaned_sentence = [(lm.lemmatize(word.text), word.pos_) for word in sentence if word.pos_ in VALID_POS]\n",
    "    for i in range(len(cleaned_sentence)):\n",
    "        word,pos = cleaned_sentence[i]\n",
    "        if word not in model: continue\n",
    "        val = model[word][1]\n",
    "        pos = remap_pos(pos, val)\n",
    "        if i != 0:\n",
    "            prev_word, prev_pos = cleaned_sentence[i-1]\n",
    "            if prev_word not in model: continue\n",
    "            prev_val = model[prev_word][1]\n",
    "            prev_pos = remap_pos(prev_pos, prev_val)\n",
    "            transition[prev_pos][pos] += 1\n",
    "        \n",
    "        emission[pos][word] = emission[pos].get(word, 0) + 1\n",
    "\n",
    "for key in transition:\n",
    "    normalize(transition[key])\n",
    "\n",
    "for key in emission:\n",
    "    normalize(emission[key])\n",
    "\n",
    "transition        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8e2dd30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Baum-Welch approximation for Matrices\"\"\"\n",
    "\n",
    "def distribute(values, num_partitions):\n",
    "    if num_partitions <= len(values):\n",
    "        splits = np.array_split(np.array(values), num_partitions)\n",
    "        return [ i for i in range(len(splits)) for j in range(len(splits[i])) ]\n",
    "    else:\n",
    "        return [ random.choice(range(num_partitions)) for value in values ]\n",
    "\n",
    "def forward(values, tr, em, initial_dist):\n",
    "    alpha = { x: {} for x in range(len(values)) }\n",
    "\n",
    "    for s in tr.keys():\n",
    "        alpha[0][s] = initial_dist[s] * em[s][values[0]]\n",
    "    for t in range(1, len(values)):\n",
    "        for s in tr.keys():\n",
    "            alpha[t][s] = sum([ alpha[t-1][s] * tr[s_p][s] * em[s][values[t]] for s_p in tr.keys() ])\n",
    "\n",
    "    return alpha\n",
    "\n",
    "def backward(values, tr, em):\n",
    "    beta = [ {} for t in range(len(values)) ] \n",
    "    beta[-1] = { s: 1 for s in tr.keys() }\n",
    "    \n",
    "    for t in range(len(values) - 2, -1, -1):\n",
    "        for s in tr.keys():\n",
    "            beta[t][s] = sum([ beta[t+1][s_p] * tr[s][s_p] * em[s_p][values[t+1]] for s_p in tr.keys() ]) \n",
    "    return beta\n",
    "\n",
    "def normalize_model(pi, tr, em):\n",
    "    normalize(pi)\n",
    "    for s in tr.keys():\n",
    "        normalize(tr[s])\n",
    "    \n",
    "    for s in em.keys():\n",
    "        normalize(em[s])\n",
    "        \n",
    "def initialize(tweets, num_states):\n",
    "    # initialize matrices\n",
    "    tr = {}\n",
    "    em = {} \n",
    "    pi = {}\n",
    "    states = []\n",
    "    \n",
    "    for i in range(num_states):\n",
    "        s_i = 's_{0}'.format(i)\n",
    "        states.append(s_i)\n",
    "        # initialize tr as random\n",
    "        tr[s_i] = {}\n",
    "        for j in range(num_states):\n",
    "            s_j = 's_{0}'.format(j)\n",
    "            tr[s_i][s_j] = random.random()\n",
    "        normalize(tr[s_i])\n",
    "            \n",
    "        # also initialize em num of states\n",
    "        em[s_i] = {}\n",
    "        \n",
    "        # also initialize initial distribution\n",
    "        pi[s_i] = 1.0 / num_states\n",
    "        \n",
    "    for tweet in tweets:\n",
    "        words = [lm.lemmatize(word) for word in tweet.split()]\n",
    "        distributed = distribute(words, num_states)\n",
    "        \n",
    "        for (word, i) in zip(words, distributed):\n",
    "            s_i = 's_{0}'.format(i)\n",
    "            if word not in em[s_i]:\n",
    "                for j in range(num_states):\n",
    "                    s_j = 's_{0}'.format(j)\n",
    "                    em[s_j][word] = 0\n",
    "            em[s_i][word] += 1\n",
    "            \n",
    "    for s in em:\n",
    "        normalize(em[s])\n",
    "        \n",
    "    return (tr, em, pi)\n",
    "\n",
    "def bw(tweets, num_states=4, num_iter=10, init=None):\n",
    "    if init is None:\n",
    "        tr, em, pi = initialize(tweets, num_states)\n",
    "    else:\n",
    "        tr, em, pi = init\n",
    "    \n",
    "    states = tr.keys()\n",
    "    \n",
    "    # iterate to improve values\n",
    "    for it in range(num_iter):\n",
    "        start_time = time.time()\n",
    "        # improve tr and em\n",
    "        R = []\n",
    "        for tweet in tweets:\n",
    "            words = [lm.lemmatize(word) for word in tweet.split()]\n",
    "            if len(words) == 0: continue\n",
    "\n",
    "            # estimation\n",
    "            alpha = forward(words, tr, em, pi)\n",
    "            beta = backward(words, tr, em)\n",
    "\n",
    "            # E step\n",
    "            xi = [ { s1: { s2: 0 for s2 in states} for s1 in states } for x in range(len(words)) ]\n",
    "            gamma = [ { s1: 0 for s1 in states } for x in range(len(words)) ]\n",
    "            for t in range(len(words)):\n",
    "                gamma_denominator = sum([alpha[t][s] * beta[t][s] for s in states])\n",
    "                if t == len(words) - 1:\n",
    "                    for s1 in states:\n",
    "                        gamma[t][s1] = alpha[t][s1] * beta[t][s1] / gamma_denominator if gamma_denominator != 0 else 0\n",
    "                    continue\n",
    "\n",
    "                xi_denominator = sum([alpha[t][s1] * tr[s1][s2] * em[s2][words[t+1]] * beta[t+1][s2]\n",
    "                                      for s1 in states for s2 in states])\n",
    "\n",
    "                for s1 in states:\n",
    "                    for s2 in states:\n",
    "                        if s1 not in xi[t]: xi[t][s1] = {}\n",
    "\n",
    "                        # might want to flip alpha in forward function to match beta format\n",
    "                        xi[t][s1][s2] = alpha[t][s1] * tr[s1][s2] * em[s2][words[t+1]] * beta[t+1][s2] / xi_denominator \\\n",
    "                        if xi_denominator != 0 else 0\n",
    "\n",
    "                    gamma[t][s1] = alpha[t][s1] * beta[t][s1] / gamma_denominator if gamma_denominator != 0 else 0\n",
    "            r_values = ( words, gamma, xi )\n",
    "            R.append(r_values)\n",
    "\n",
    "        # M step\n",
    "        for s1 in states:\n",
    "            pi[s1] = sum([gamma[0][s1] for _,gamma,_ in R]) / len(R)\n",
    "\n",
    "            tr_denominator = sum([xi[t][s1][s] for s in states for (words, _, xi) in R for t in range(len(words)-1)])\n",
    "            for s2 in states:\n",
    "                tr_numerator = sum([xi[t][s1][s2] for (words, _, xi) in R for t in range(len(words)-1) ])\n",
    "                tr[s1][s2] = tr_numerator / tr_denominator if tr_denominator != 0 else 0\n",
    "\n",
    "            em_denominator = sum([gamma[t][s1] for (words, gamma, _) in R for t in range(len(words))])\n",
    "            for w in em[s1]:\n",
    "                em_numerator = sum([gamma[t][s1] for (words, gamma, _) in R for t in range(len(words)) if words[t] == w])\n",
    "                if em_denominator == 0:\n",
    "                    em[s1][w] = 0\n",
    "                else:\n",
    "                    em[s1][w] = em_numerator / em_denominator\n",
    "        normalize_model(pi, tr, em)\n",
    "        end_time = time.time()\n",
    "        print('Iteration %d Completed, Time Taken: %s' % (it, start_time - end_time))\n",
    "    return (pi, tr, em)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ea632a5-dbfd-4425-b888-6485f07e7719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(pi, tr, em, filename='model.json'):\n",
    "    with open(filename, 'w') as f:\n",
    "        model = { 'pi': pi, 'tr': tr, 'em': em }\n",
    "        f.write(json.dumps(model))\n",
    "        f.close()\n",
    "\n",
    "def read_model(filename='model.json'):\n",
    "    model = json.load(filename)\n",
    "    pi = model['pi']\n",
    "    tr = model['tr']\n",
    "    em = model['em']\n",
    "    return (pi, tr, em)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65a429af-5e66-4328-82fb-2dc654526aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 Completed, Time Taken: -2.384185791015625e-07\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ea3cf34c9dd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpos_tweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sentiment\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpos_pi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_em\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_pi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_em\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pos_model.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-b3a3aa6acae3>\u001b[0m in \u001b[0;36mbw\u001b[0;34m(tweets, num_states, num_iter, init)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mem_denominator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mR\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 \u001b[0mem_numerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mR\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mem_denominator\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                     \u001b[0mem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-b3a3aa6acae3>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mem_denominator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mR\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 \u001b[0mem_numerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mR\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mem_denominator\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                     \u001b[0mem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\" First time run for BW -- Future runs can iterate directly on model \"\"\"\n",
    "\n",
    "pos_tweets = X_train[X_train[\"sentiment\"] == 1]\n",
    "pos_pi, pos_tr, pos_em = bw(pos_tweets['tweet'])\n",
    "save_model(pos_pi, pos_tr, pos_em, 'pos_model.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "31a67fee",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'max_prob' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wx/k0pkmhxx13j_cnk_rv7qt19m0000gn/T/ipykernel_85541/2775537746.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_tweet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_tweet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_tweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mviterbi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_tweet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbw_pi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbw_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbw_em\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/wx/k0pkmhxx13j_cnk_rv7qt19m0000gn/T/ipykernel_85541/859704934.py\u001b[0m in \u001b[0;36mviterbi\u001b[0;34m(obs, states, start_p, trans_p, emit_p)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0memit_p\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0mmax_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_tr_prob\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0memit_p\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mV\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"prob\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmax_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"prev\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprev_st_selected\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'max_prob' referenced before assignment"
     ]
    }
   ],
   "source": [
    "states = ('s_0', 's_1', 's_2', 's_3')\n",
    "possible_observations = model.keys()\n",
    "test_tweet = df['tweet'][0]\n",
    "test_tweet = [ lm.lemmatize(word) for word in test_tweet.split()]\n",
    "viterbi(test_tweet, states, bw_pi, bw_tr, bw_em)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4a70f1-c627-4269-8c19-ccaa14c94847",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"OLD HMM MODEL\"\"\"\n",
    "\n",
    "# transition_prob = {\n",
    "#     'neg': { 'pos': 0, 'neg': 0 },\n",
    "#     'pos': { 'pos': 0, 'neg': 0 }\n",
    "# }\n",
    "# emission_prob = {\n",
    "#     'neg': { },\n",
    "#     'pos': { }\n",
    "# }\n",
    "\n",
    "def state_to_num(s):\n",
    "    return 1 if s == 'pos' else 0\n",
    "\n",
    "def num_to_state(n):\n",
    "    return 'pos' if n >= 0.5 else 'neg'\n",
    "\n",
    "# for sentiment, tweet in zip(X_train['sentiment'], X_train['tweet']):\n",
    "#     words = [lm.lemmatize(word) for word in tweet.split()]\n",
    "    \n",
    "#     for i in range(len(words)): \n",
    "#         wovrd = words[i]\n",
    "        \n",
    "#         # transition stuff\n",
    "#         #if i != 0:\n",
    "#         #    val = model[word][1]\n",
    "#         #    current_state = num_to_state(val)\n",
    "#         #    previous_state = num_to_state(model[words[i-1]][1])\n",
    "#         #    \n",
    "#         #    transition_prob[previous_state][current_state] += 1\n",
    "        \n",
    "#         actual_state = num_to_state(sentiment)    \n",
    "#         word_state = num_to_state(model[word][1])\n",
    "        \n",
    "#         # transition stuff\n",
    "#         transition_prob[word_state][actual_state] += 1        \n",
    "    \n",
    "#         # emission stuff\n",
    "#         if word not in emission_prob[actual_state]:\n",
    "#             emission_prob['pos'][word] = 0\n",
    "#             emission_prob['neg'][word] = 0\n",
    "#         emission_prob[actual_state][word] += 1    \n",
    "        \n",
    "# normalize -- subtract mean and divide by std\n",
    "# min-max scaling\n",
    "# add-one smoothing for naive bayes\n",
    "            \n",
    "# # normalize values in transition\n",
    "# normalize(transition_prob['pos'])\n",
    "# normalize(transition_prob['neg'])\n",
    "# # normalize value in emission\n",
    "# normalize(emission_prob['pos'])\n",
    "# normalize(emission_prob['neg'])\n",
    "\n",
    "# (transition_prob, emission_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3099a4d-fe6b-449a-b19b-cd6f13d4aaa5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ratios = { }     \n",
    "for sentiment, tweet in zip(X_train['sentiment'], X_train['tweet']):\n",
    "    words = [lm.lemmatize(word) for word in tweet.split()]\n",
    "    vals = [model[word][1] for word in words if word in model]\n",
    "    \n",
    "    pos_count = len(list(filter(lambda v : v >= 0.5, vals)))\n",
    "    neg_count = len(list(filter(lambda v : v < 0.5, vals)))\n",
    "    \n",
    "    ratio = 0.5 if (pos_count + neg_count) == 0 else round(pos_count * 1.0 / (pos_count + neg_count), 3)\n",
    "    \n",
    "    if ratio in ratios:\n",
    "        count, avg = ratios[ratio]\n",
    "        ratios[ratio] = (count+1, (count*avg+sentiment) / (count+1))\n",
    "    else:\n",
    "        ratios[ratio] = (1, sentiment)\n",
    "\n",
    "ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fb8e76-b9e5-444c-9183-24e2c5a07306",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def estimator(tweet):\n",
    "    words = [lm.lemmatize(word) for word in tweet.split()]\n",
    "    vals = [model[word][1] for word in words if word in model]\n",
    "    \n",
    "    if len(vals) == 0:\n",
    "        return 0.5\n",
    "    \n",
    "    return sum(vals)/len(vals)\n",
    "\n",
    "def approximate_ratio(ratio):\n",
    "    rkeys = sorted(ratios.keys())\n",
    "\n",
    "    if ratio < rkeys[0]:\n",
    "        return ratios[rkeys[0]][1]\n",
    "    \n",
    "    if ratio > rkeys[-1]:\n",
    "        return ratios[rkeys[-1]][1]\n",
    "    \n",
    "    l = 0\n",
    "    while ratio > rkeys[l]:\n",
    "        l += 1\n",
    "                      \n",
    "    return (ratios[rkeys[l]][1] + ratios[rkeys[l+1]][1])/2\n",
    "\n",
    "def ratio_estimator(tweet):\n",
    "    words = [lm.lemmatize(word) for word in tweet.split()]\n",
    "    vals = [model[word][1] for word in words if word in model]\n",
    "    \n",
    "    pos_count = len(list(filter(lambda v : v >= 0.5, vals)))\n",
    "    neg_count = len(list(filter(lambda v : v < 0.5, vals)))\n",
    "    \n",
    "    ratio = 0.5 if (pos_count + neg_count) == 0 else round(pos_count * 1.0 / (pos_count + neg_count), 3)\n",
    "    \n",
    "    if ratio not in ratios:\n",
    "        return approximate_ratio(ratio)\n",
    "        #ratios[ratio] = (1, approximate_ratio(ratio))\n",
    "        \n",
    "    return ratios[ratio][1]\n",
    "\n",
    "def hmm_estimator(tweet):\n",
    "    words = [lm.lemmatize(word) for word in tweet.split()]\n",
    "    \n",
    "    tr = transition_prob\n",
    "    em = emission_prob\n",
    "       \n",
    "    gl = { 'pos': 0.5, 'neg': 0.5 }\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        word = words[i]\n",
    "        if word not in model:\n",
    "            continue\n",
    "            \n",
    "        word_state = num_to_state(model[word][1])\n",
    "        \n",
    "        pr = {\n",
    "            'pos': tr['pos']['pos'] * gl['pos'] + tr['neg']['pos'] * gl['neg'],\n",
    "            'neg': tr['pos']['neg'] * gl['pos'] + tr['neg']['neg'] * gl['neg']\n",
    "        }\n",
    "        \n",
    "        gl['pos'] = em['pos'][word] * pr['pos']\n",
    "        gl['neg'] = em['neg'][word] * pr['neg']\n",
    "        \n",
    "        # just doing this results in 69% accuracy.. but not sure if this is right.\n",
    "        #gl['pos'] = tr[word_state]['pos'] * gl['pos']\n",
    "        #gl['neg'] = tr[word_state]['neg'] * gl['neg']\n",
    "        \n",
    "        normalize(gl)\n",
    "    \n",
    "    return gl['pos']\n",
    "\n",
    "sentiment_pairs = []\n",
    "for sentiment, tweet in zip(X_test['sentiment'], X_test['tweet']):\n",
    "    estimated_sentiment = estimator(tweet)\n",
    "    sentiment_pairs.append((sentiment, estimated_sentiment))\n",
    "                      \n",
    "ratio_sentiment_pairs = []\n",
    "for sentiment, tweet in zip(X_test['sentiment'], X_test['tweet']):\n",
    "    estimated_sentiment = ratio_estimator(tweet)\n",
    "    ratio_sentiment_pairs.append((sentiment, estimated_sentiment))\n",
    "    \n",
    "hmm_sentiment_pairs = []\n",
    "for sentiment, tweet in zip(X_test['sentiment'], X_test['tweet']):\n",
    "    estimated_sentiment = hmm_estimator(tweet)\n",
    "    hmm_sentiment_pairs.append((sentiment, estimated_sentiment))\n",
    "\n",
    "sentiment_pairs = hmm_sentiment_pairs\n",
    "\n",
    "sentiment_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7354df-4180-4388-ad7d-405dcdb3bdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_errors(sentiment_pairs):\n",
    "    errors = [abs(est - sent) for (sent, est) in sentiment_pairs]\n",
    "    avg_error = sum(errors)/len(errors)\n",
    "    var_values = [(error - avg_error)**2 for error in errors]\n",
    "    variance = sum(var_values)/len(var_values)\n",
    "\n",
    "    return (avg_error, variance)\n",
    "    \n",
    "\n",
    "avg_error, variance = compute_errors(sentiment_pairs)\n",
    "    \n",
    "print('Average Error:', avg_error)\n",
    "print('Variance:', variance)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b3321b-f33a-433b-a1e4-f9e0cc5530d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded_sentiment_pairs = [(sent, 0 if est < 0.5 else 1) for (sent, est) in sentiment_pairs]\n",
    "\n",
    "# f1 metrics\n",
    "accuracy = len([1 for (sent, est) in rounded_sentiment_pairs if sent == est]) / len(rounded_sentiment_pairs)\n",
    "\n",
    "avg_error, variance = compute_errors(rounded_sentiment_pairs)\n",
    "\n",
    "print('Accuracy:', accuracy)\n",
    "print('Average Error:', avg_error)\n",
    "print('Variance:', variance)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59451a61-d207-4356-8610-80e4c8fed70c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "fp = open(\"test.txt\")\n",
    "data = fp.read()\n",
    "paragraphs = data.split('\\n\\n')\n",
    "paragraph_sentences = [ '\\n-----\\n'.join(tokenizer.tokenize(paragraph)).split('\\n-----\\n') for paragraph in paragraphs ]\n",
    "\n",
    "paragraph_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e763a3-0687-43fe-9da9-0092f7b7eea9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "paragraph_values = [ [ estimator(sentence) for sentence in paragraph ] for paragraph in paragraph_sentences ]\n",
    "averaged_values = [ sum(paragraph)/len(paragraph) for paragraph in paragraph_values ]\n",
    "\n",
    "averaged_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fdc9ce-1f93-4e6e-bf6c-19ddcb9e4ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(range(len(averaged_values)),averaged_values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a9b704-e4b5-4ab2-971a-1851d28b8fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage.filters import gaussian_filter1d\n",
    "\n",
    "ysmoothed = gaussian_filter1d(averaged_values, sigma=1)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(range(len(averaged_values)), ysmoothed)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be865c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
