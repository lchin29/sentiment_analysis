{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f1df22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. dataset of sentences to overall sentiment\n",
    "2. create a model from dataset of word to sentiment\n",
    "2.5 gramatically break down sentence instead of word by word\n",
    "3. use model for our own sentence input to output overall sentiment of sentence\n",
    "3.5 if a word is not known --> do sentiment analysis on its dictionary definition or neutralize\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6d84cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk.data\n",
    "import spacy\n",
    "import json\n",
    "import random\n",
    "# need to run first 'python -m spacy download en_core_web_sm'\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "\n",
    "lm = WordNetLemmatizer()\n",
    "\n",
    "def normalize(d):\n",
    "    mult = 1.0/sum(d.values())\n",
    "    for key in d:\n",
    "        d[key] *= mult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9abba92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lindseychin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lindseychin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "704c14bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [\"sentiment\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "df = pd.read_csv(\n",
    "    'training.1600000.processed.noemoticon.csv',\n",
    "    header=None, encoding=\"ISO-8859-1\", names=col_names)\n",
    "\n",
    "# normalize to 0 to 1 values\n",
    "df['sentiment'] = df['sentiment'].replace(4, 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, df['sentiment'], test_size = 0.10, random_state=0)\n",
    "\n",
    "df = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fac3fa9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NEGATION_WORDS = ['not', 'no']\n",
    "STOP_WORDS = stopwords.words('english')\n",
    "\n",
    "for word in NEGATION_WORDS:\n",
    "    STOP_WORDS.remove(word)\n",
    "\n",
    "URL_PATTERN = r'((https://[^ ]*|(http://)[^ ]*|( www\\.)[^ ]*))'\n",
    "USER_PATTERN = '@[^\\s]+'\n",
    "PUNCTUATIONS = ['!', '?', '&quot;']\n",
    "\n",
    "processed = []\n",
    "\n",
    "# NOTE TO SELF: REMOVE PUNCTUATION BC SPACY\n",
    "for sentiment, tweet in zip(df['sentiment'], df['text']):\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(URL_PATTERN, '', tweet)\n",
    "    tweet = re.sub(USER_PATTERN, '', tweet)\n",
    "    for p in PUNCTUATIONS:\n",
    "        tweet = tweet.replace(p, '')\n",
    "    for sw in STOP_WORDS:\n",
    "        tweet = re.sub(r'\\b{0}\\b'.format(sw), '', tweet)\n",
    "    for w in NEGATION_WORDS:\n",
    "        tweet = re.sub(r'\\b{0} \\b'.format(w), '{0}_'.format(w), tweet)\n",
    "        \n",
    "    processed.append((sentiment, tweet))    \n",
    "    \n",
    "df = pd.DataFrame(data=processed, columns=['sentiment', 'tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b9c7e38",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"NAIVE MODEL\"\"\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, df['sentiment'], test_size = 0.05, random_state=0)\n",
    "\n",
    "model = {}\n",
    "for sentiment, tweet in zip(X_train['sentiment'], X_train['tweet']):\n",
    "    for word in tweet.split():\n",
    "        word = lm.lemmatize(word)\n",
    "        if word in model:\n",
    "            count, avg = model[word]\n",
    "            model[word] = (count+1, (count*avg+sentiment) / (count+1))\n",
    "        else:\n",
    "            model[word] = (1, sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95576816",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"SUPERVISED TRANSITON + EMISSION MATRICES\"\"\"\n",
    "pos_tweets = X_train[X_train[\"sentiment\"] == 1]\n",
    "transition = {\n",
    "    'NOUN': { 'NOUN': 0, 'POS_VERB': 0, 'POS_ADJ': 0, 'NEG_VERB': 0, 'NEG_ADJ': 0},\n",
    "    'POS_VERB': { 'NOUN': 0, 'POS_VERB': 0, 'POS_ADJ': 0, 'NEG_VERB': 0, 'NEG_ADJ': 0},\n",
    "    'POS_ADJ': { 'NOUN': 0, 'POS_VERB': 0, 'POS_ADJ': 0, 'NEG_VERB': 0, 'NEG_ADJ': 0},\n",
    "    'NEG_VERB': { 'NOUN': 0, 'POS_VERB': 0, 'POS_ADJ': 0, 'NEG_VERB': 0, 'NEG_ADJ': 0},\n",
    "    'NEG_ADJ': { 'NOUN': 0, 'POS_VERB': 0, 'POS_ADJ': 0, 'NEG_VERB': 0, 'NEG_ADJ': 0}\n",
    "}\n",
    "\n",
    "emission = {\n",
    "    'NOUN': {},\n",
    "    'POS_VERB': {},\n",
    "    'POS_ADJ': {},\n",
    "    'NEG_VERB': {},\n",
    "    'NEG_ADJ': {}\n",
    "}\n",
    "\n",
    "VALID_POS = set(['NOUN', 'ADJ', 'VERB', 'PRON', 'PROPN'])\n",
    "\n",
    "#name pending\n",
    "def remap_pos(pos, val):\n",
    "    if pos == 'NOUN' or pos == 'PRON' or pos == 'PROPN':\n",
    "        return 'NOUN'\n",
    "    prefix = 'POS_' if val >= 0.5 else 'NEG_'\n",
    "    return prefix + pos\n",
    "\n",
    "for tweet in pos_tweets['tweet']:\n",
    "    sentence = sp(tweet)\n",
    "    cleaned_sentence = [(lm.lemmatize(word.text), word.pos_) for word in sentence if word.pos_ in VALID_POS]\n",
    "    for i in range(len(cleaned_sentence)):\n",
    "        word,pos = cleaned_sentence[i]\n",
    "        if word not in model: continue\n",
    "        val = model[word][1]\n",
    "        pos = remap_pos(pos, val)\n",
    "        if i != 0:\n",
    "            prev_word, prev_pos = cleaned_sentence[i-1]\n",
    "            if prev_word not in model: continue\n",
    "            prev_val = model[prev_word][1]\n",
    "            prev_pos = remap_pos(prev_pos, prev_val)\n",
    "            transition[prev_pos][pos] += 1\n",
    "        \n",
    "        emission[pos][word] = emission[pos].get(word, 0) + 1\n",
    "\n",
    "for key in transition:\n",
    "    normalize(transition[key])\n",
    "\n",
    "for key in emission:\n",
    "    normalize(emission[key])\n",
    "\n",
    "transition        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e2dd30",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Baum-Welch approximation for Matrices\"\"\"\n",
    "\n",
    "def distribute(values, num_partitions):\n",
    "    if num_partitions <= len(values):\n",
    "        splits = np.array_split(np.array(values), num_partitions)\n",
    "        return [ i for i in range(len(splits)) for j in range(len(splits[i])) ]\n",
    "    else:\n",
    "        return [ random.choice(range(num_partitions)) for value in values ]\n",
    "\n",
    "def forward(values, tr, em, initial_dist):\n",
    "    alpha = { x: {} for x in range(len(values)) }\n",
    "\n",
    "    for s in tr.keys():\n",
    "        alpha[0][s] = initial_dist[s] * em[s][values[0]]\n",
    "    for t in range(1, len(values)):\n",
    "        for s in tr.keys():\n",
    "            alpha[t][s] = sum([ alpha[t-1][s] * tr[s_p][s] * em[s][values[t]] for s_p in tr.keys() ])\n",
    "\n",
    "    return alpha\n",
    "\n",
    "def backward(values, tr, em):\n",
    "    beta = [ {} for t in range(len(values)) ] \n",
    "    beta[-1] = { s: 1 for s in tr.keys() }\n",
    "    \n",
    "    for t in range(len(values) - 2, -1, -1):\n",
    "        for s in tr.keys():\n",
    "            beta[t][s] = sum([ beta[t+1][s_p] * tr[s][s_p] * em[s_p][values[t+1]] for s_p in tr.keys() ]) \n",
    "    return beta\n",
    "\n",
    "def normalize_tr_em(tr, em):\n",
    "    print(tr)\n",
    "    for s in tr.keys():\n",
    "        normalize(tr[s])\n",
    "    \n",
    "    for s in em.keys():\n",
    "        normalize(em[s])\n",
    "\n",
    "def bw(tweets, num_states=4):\n",
    "    # initialize matrices\n",
    "    tr = {}\n",
    "    em = {} \n",
    "    initial_distribution = {}\n",
    "    states = []\n",
    "    \n",
    "    for i in range(num_states):\n",
    "        s_i = 's_{0}'.format(i)\n",
    "        states.append(s_i)\n",
    "        # initialize tr as random\n",
    "        tr[s_i] = {}\n",
    "        for j in range(num_states):\n",
    "            s_j = 's_{0}'.format(j)\n",
    "            tr[s_i][s_j] = random.random()\n",
    "        normalize(tr[s_i])\n",
    "            \n",
    "        # also initialize em num of states\n",
    "        em[s_i] = {}\n",
    "        \n",
    "        # also initialize initial distribution\n",
    "        initial_distribution[s_i] = 1.0 / num_states\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        words = [lm.lemmatize(word) for word in tweet.split()]\n",
    "        distributed = distribute(words, num_states)\n",
    "        \n",
    "        for (word, i) in zip(words, distributed):\n",
    "            s_i = 's_{0}'.format(i)\n",
    "            if word not in em[s_i]:\n",
    "                for j in range(num_states):\n",
    "                    s_j = 's_{0}'.format(j)\n",
    "                    em[s_j][word] = 0\n",
    "            em[s_i][word] += 1\n",
    "            \n",
    "    for s in em:\n",
    "        normalize(em[s])\n",
    "        \n",
    "    # improve tr and em\n",
    "    R = []\n",
    "    for tweet in tweets:\n",
    "        words = [lm.lemmatize(word) for word in tweet.split()]\n",
    "        if len(words) == 0: continue\n",
    "        \n",
    "        # estimation\n",
    "        alpha = forward(words, tr, em, initial_distribution)\n",
    "        beta = backward(words, tr, em)\n",
    "        \n",
    "        # E step\n",
    "        xi = [ { s1: { s2: 0 for s2 in states} for s1 in states } for x in range(len(words)) ]\n",
    "        gamma = [ { s1: 0 for s1 in states } for x in range(len(words)) ]\n",
    "        for t in range(len(words)):\n",
    "            gamma_denominator = sum([alpha[t][s] * beta[t][s] for s in states])\n",
    "            if t == len(words) - 1:\n",
    "                for s1 in states:\n",
    "                    gamma[t][s1] = alpha[t][s1] * beta[t][s1] / gamma_denominator if gamma_denominator != 0 else 0\n",
    "                continue\n",
    "                \n",
    "            xi_denominator = sum([alpha[t][s1] * tr[s1][s2] * em[s2][words[t+1]] * beta[t+1][s2]\n",
    "                                  for s1 in states for s2 in states])\n",
    "            \n",
    "            for s1 in states:\n",
    "                for s2 in states:\n",
    "                    if s1 not in xi[t]: xi[t][s1] = {}\n",
    "\n",
    "                    # might want to flip alpha in forward function to match beta format\n",
    "                    xi[t][s1][s2] = alpha[t][s1] * tr[s1][s2] * em[s2][words[t+1]] * beta[t+1][s2] / xi_denominator \\\n",
    "                    if xi_denominator != 0 else 0\n",
    "                \n",
    "                gamma[t][s1] = alpha[t][s1] * beta[t][s1] / gamma_denominator if gamma_denominator != 0 else 0\n",
    "        r_values = ( words, gamma, xi )\n",
    "        R.append(r_values)\n",
    "        \n",
    "    # M step\n",
    "    for s1 in states:\n",
    "        tr_denominator = sum([xi[t][s1][s] for s in states for (words, _, xi) in R for t in range(len(words)-1)])\n",
    "        for s2 in states:\n",
    "            tr_numerator = sum([xi[t][s1][s2] for (words, _, xi) in R for t in range(len(words)-1) ])\n",
    "            tr[s1][s2] = tr_numerator / tr_denominator if tr_denominator != 0 else 0\n",
    "\n",
    "        em_denominator = sum([gamma[t][s1] for (words, gamma, _) in R for t in range(len(words))])\n",
    "        for w in em[s1]:\n",
    "            em_numerator = sum([gamma[t][s1] for (words, gamma, _) in R for t in range(len(words)) if words[t] == w])\n",
    "            if em_denominator == 0:\n",
    "                em[s1][w] = 0\n",
    "            else:\n",
    "                em[s1][w] = em_numerator / em_denominator\n",
    "\n",
    "    normalize_tr_em(tr, em)\n",
    "    return (tr, em)\n",
    "\n",
    "pos_tweets = X_train[X_train[\"sentiment\"] == 1]\n",
    "bw_tr, bw_em = bw(pos_tweets['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4a70f1-c627-4269-8c19-ccaa14c94847",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"OLD HMM MODEL\"\"\"\n",
    "\n",
    "# transition_prob = {\n",
    "#     'neg': { 'pos': 0, 'neg': 0 },\n",
    "#     'pos': { 'pos': 0, 'neg': 0 }\n",
    "# }\n",
    "# emission_prob = {\n",
    "#     'neg': { },\n",
    "#     'pos': { }\n",
    "# }\n",
    "\n",
    "def state_to_num(s):\n",
    "    return 1 if s == 'pos' else 0\n",
    "\n",
    "def num_to_state(n):\n",
    "    return 'pos' if n >= 0.5 else 'neg'\n",
    "\n",
    "# for sentiment, tweet in zip(X_train['sentiment'], X_train['tweet']):\n",
    "#     words = [lm.lemmatize(word) for word in tweet.split()]\n",
    "    \n",
    "#     for i in range(len(words)): \n",
    "#         wovrd = words[i]\n",
    "        \n",
    "#         # transition stuff\n",
    "#         #if i != 0:\n",
    "#         #    val = model[word][1]\n",
    "#         #    current_state = num_to_state(val)\n",
    "#         #    previous_state = num_to_state(model[words[i-1]][1])\n",
    "#         #    \n",
    "#         #    transition_prob[previous_state][current_state] += 1\n",
    "        \n",
    "#         actual_state = num_to_state(sentiment)    \n",
    "#         word_state = num_to_state(model[word][1])\n",
    "        \n",
    "#         # transition stuff\n",
    "#         transition_prob[word_state][actual_state] += 1        \n",
    "    \n",
    "#         # emission stuff\n",
    "#         if word not in emission_prob[actual_state]:\n",
    "#             emission_prob['pos'][word] = 0\n",
    "#             emission_prob['neg'][word] = 0\n",
    "#         emission_prob[actual_state][word] += 1    \n",
    "        \n",
    "# normalize -- subtract mean and divide by std\n",
    "# min-max scaling\n",
    "# add-one smoothing for naive bayes\n",
    "            \n",
    "# # normalize values in transition\n",
    "# normalize(transition_prob['pos'])\n",
    "# normalize(transition_prob['neg'])\n",
    "# # normalize value in emission\n",
    "# normalize(emission_prob['pos'])\n",
    "# normalize(emission_prob['neg'])\n",
    "\n",
    "# (transition_prob, emission_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3099a4d-fe6b-449a-b19b-cd6f13d4aaa5",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ratios = { }     \n",
    "for sentiment, tweet in zip(X_train['sentiment'], X_train['tweet']):\n",
    "    words = [lm.lemmatize(word) for word in tweet.split()]\n",
    "    vals = [model[word][1] for word in words if word in model]\n",
    "    \n",
    "    pos_count = len(list(filter(lambda v : v >= 0.5, vals)))\n",
    "    neg_count = len(list(filter(lambda v : v < 0.5, vals)))\n",
    "    \n",
    "    ratio = 0.5 if (pos_count + neg_count) == 0 else round(pos_count * 1.0 / (pos_count + neg_count), 3)\n",
    "    \n",
    "    if ratio in ratios:\n",
    "        count, avg = ratios[ratio]\n",
    "        ratios[ratio] = (count+1, (count*avg+sentiment) / (count+1))\n",
    "    else:\n",
    "        ratios[ratio] = (1, sentiment)\n",
    "\n",
    "ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fb8e76-b9e5-444c-9183-24e2c5a07306",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def estimator(tweet):\n",
    "    words = [lm.lemmatize(word) for word in tweet.split()]\n",
    "    vals = [model[word][1] for word in words if word in model]\n",
    "    \n",
    "    if len(vals) == 0:\n",
    "        return 0.5\n",
    "    \n",
    "    return sum(vals)/len(vals)\n",
    "\n",
    "def approximate_ratio(ratio):\n",
    "    rkeys = sorted(ratios.keys())\n",
    "\n",
    "    if ratio < rkeys[0]:\n",
    "        return ratios[rkeys[0]][1]\n",
    "    \n",
    "    if ratio > rkeys[-1]:\n",
    "        return ratios[rkeys[-1]][1]\n",
    "    \n",
    "    l = 0\n",
    "    while ratio > rkeys[l]:\n",
    "        l += 1\n",
    "                      \n",
    "    return (ratios[rkeys[l]][1] + ratios[rkeys[l+1]][1])/2\n",
    "\n",
    "def ratio_estimator(tweet):\n",
    "    words = [lm.lemmatize(word) for word in tweet.split()]\n",
    "    vals = [model[word][1] for word in words if word in model]\n",
    "    \n",
    "    pos_count = len(list(filter(lambda v : v >= 0.5, vals)))\n",
    "    neg_count = len(list(filter(lambda v : v < 0.5, vals)))\n",
    "    \n",
    "    ratio = 0.5 if (pos_count + neg_count) == 0 else round(pos_count * 1.0 / (pos_count + neg_count), 3)\n",
    "    \n",
    "    if ratio not in ratios:\n",
    "        return approximate_ratio(ratio)\n",
    "        #ratios[ratio] = (1, approximate_ratio(ratio))\n",
    "        \n",
    "    return ratios[ratio][1]\n",
    "\n",
    "def hmm_estimator(tweet):\n",
    "    words = [lm.lemmatize(word) for word in tweet.split()]\n",
    "    \n",
    "    tr = transition_prob\n",
    "    em = emission_prob\n",
    "       \n",
    "    gl = { 'pos': 0.5, 'neg': 0.5 }\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        word = words[i]\n",
    "        if word not in model:\n",
    "            continue\n",
    "            \n",
    "        word_state = num_to_state(model[word][1])\n",
    "        \n",
    "        pr = {\n",
    "            'pos': tr['pos']['pos'] * gl['pos'] + tr['neg']['pos'] * gl['neg'],\n",
    "            'neg': tr['pos']['neg'] * gl['pos'] + tr['neg']['neg'] * gl['neg']\n",
    "        }\n",
    "        \n",
    "        gl['pos'] = em['pos'][word] * pr['pos']\n",
    "        gl['neg'] = em['neg'][word] * pr['neg']\n",
    "        \n",
    "        # just doing this results in 69% accuracy.. but not sure if this is right.\n",
    "        #gl['pos'] = tr[word_state]['pos'] * gl['pos']\n",
    "        #gl['neg'] = tr[word_state]['neg'] * gl['neg']\n",
    "        \n",
    "        normalize(gl)\n",
    "    \n",
    "    return gl['pos']\n",
    "\n",
    "sentiment_pairs = []\n",
    "for sentiment, tweet in zip(X_test['sentiment'], X_test['tweet']):\n",
    "    estimated_sentiment = estimator(tweet)\n",
    "    sentiment_pairs.append((sentiment, estimated_sentiment))\n",
    "                      \n",
    "ratio_sentiment_pairs = []\n",
    "for sentiment, tweet in zip(X_test['sentiment'], X_test['tweet']):\n",
    "    estimated_sentiment = ratio_estimator(tweet)\n",
    "    ratio_sentiment_pairs.append((sentiment, estimated_sentiment))\n",
    "    \n",
    "hmm_sentiment_pairs = []\n",
    "for sentiment, tweet in zip(X_test['sentiment'], X_test['tweet']):\n",
    "    estimated_sentiment = hmm_estimator(tweet)\n",
    "    hmm_sentiment_pairs.append((sentiment, estimated_sentiment))\n",
    "\n",
    "sentiment_pairs = hmm_sentiment_pairs\n",
    "\n",
    "sentiment_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7354df-4180-4388-ad7d-405dcdb3bdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_errors(sentiment_pairs):\n",
    "    errors = [abs(est - sent) for (sent, est) in sentiment_pairs]\n",
    "    avg_error = sum(errors)/len(errors)\n",
    "    var_values = [(error - avg_error)**2 for error in errors]\n",
    "    variance = sum(var_values)/len(var_values)\n",
    "\n",
    "    return (avg_error, variance)\n",
    "    \n",
    "\n",
    "avg_error, variance = compute_errors(sentiment_pairs)\n",
    "    \n",
    "print('Average Error:', avg_error)\n",
    "print('Variance:', variance)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b3321b-f33a-433b-a1e4-f9e0cc5530d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded_sentiment_pairs = [(sent, 0 if est < 0.5 else 1) for (sent, est) in sentiment_pairs]\n",
    "\n",
    "# f1 metrics\n",
    "accuracy = len([1 for (sent, est) in rounded_sentiment_pairs if sent == est]) / len(rounded_sentiment_pairs)\n",
    "\n",
    "avg_error, variance = compute_errors(rounded_sentiment_pairs)\n",
    "\n",
    "print('Accuracy:', accuracy)\n",
    "print('Average Error:', avg_error)\n",
    "print('Variance:', variance)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59451a61-d207-4356-8610-80e4c8fed70c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "fp = open(\"test.txt\")\n",
    "data = fp.read()\n",
    "paragraphs = data.split('\\n\\n')\n",
    "paragraph_sentences = [ '\\n-----\\n'.join(tokenizer.tokenize(paragraph)).split('\\n-----\\n') for paragraph in paragraphs ]\n",
    "\n",
    "paragraph_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e763a3-0687-43fe-9da9-0092f7b7eea9",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "paragraph_values = [ [ estimator(sentence) for sentence in paragraph ] for paragraph in paragraph_sentences ]\n",
    "averaged_values = [ sum(paragraph)/len(paragraph) for paragraph in paragraph_values ]\n",
    "\n",
    "averaged_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fdc9ce-1f93-4e6e-bf6c-19ddcb9e4ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(range(len(averaged_values)),averaged_values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a9b704-e4b5-4ab2-971a-1851d28b8fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage.filters import gaussian_filter1d\n",
    "\n",
    "ysmoothed = gaussian_filter1d(averaged_values, sigma=1)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(range(len(averaged_values)), ysmoothed)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cb9961-b457-431c-9b57-40d59b7fc4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = sp(\"Did it work?\")\n",
    "for token in s:\n",
    "    print(token.pos_)\n",
    "\n",
    "s[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be865c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
